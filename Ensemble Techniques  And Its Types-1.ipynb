{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Techniques And Its Types Assignment-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is an ensemble technique in machine learning?\n",
    "\n",
    "# Ans:\n",
    "\n",
    "# Ensemble is a technique in machine learning where we can combine multiple models to train and predict.\n",
    "# There are different types of ensemble techniques in ml, like bagging and boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. Why are ensemble techniques used in machine learning?\n",
    "\n",
    "# Ans:\n",
    "\n",
    "# Improved Accuracy: Ensemble models often achieve higher accuracy compared to single models. By combining \n",
    "# the predictions of multiple models, they can reduce errors and improve overall performance.   \n",
    "# Reduced Overfitting: Overfitting occurs when a model learns the training data too well, including its noise \n",
    "# and outliers, leading to poor performance on new data. Ensemble techniques, especially those that use \n",
    "# techniques like bagging, can help reduce overfitting by averaging out the individual model's predictions.   \n",
    "# Handling Complex Problems: Ensemble techniques can be particularly useful for complex problems where a \n",
    "# single model might struggle to capture the underlying patterns.\n",
    "# By combining multiple models, each with its own strengths, ensembles can effectively tackle these challenges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. What is bagging?\n",
    "\n",
    "# Ans:\n",
    "\n",
    "# Bagging (Bootstrap Aggregating):\n",
    "\n",
    "# Bagging is a technique when we have outputs from different models and we will take the aggregation\n",
    "# of all the model's outputs as a final output. It is like asking a bunch of different people for their \n",
    "# opinions and then taking the average.\n",
    "\n",
    "\n",
    "# Focus: Reduces variance, helps prevent overfitting.   \n",
    "# Method:\n",
    "# Creates multiple subsets of the training data by randomly sampling with replacement (bootstrapping).   \n",
    "# Trains a separate model on each subset.   \n",
    "# Combines the predictions of all models through averaging (for regression) or voting (for classification).   \n",
    "# Characteristics:\n",
    "# Models are trained independently and in parallel.   \n",
    "# Each model has equal weight in the final prediction.\n",
    "# Examples: Random Forest\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. What is boosting?\n",
    "\n",
    "# Ans:\n",
    "\n",
    "# Boosting:\n",
    "\n",
    "# Boosting is a technique when we have too many models connected sequencially and the latest maodel will trained \n",
    "# with sample data and with previous model's false predictions. It is like asking someone for their opinion, \n",
    "# then asking someone else to focus on the mistakes the first person made, and so on.\n",
    "\n",
    "# Focus: Reduces bias, improves accuracy by learning from errors.   \n",
    "# Method:\n",
    "# Trains models sequentially, with each model focusing on correcting the errors of the previous ones.   \n",
    "# Assigns weights to data points, with higher weights given to misclassified instances.   \n",
    "# Combines the predictions of all models through a weighted average or voting.\n",
    "# Characteristics:\n",
    "# Models are trained in a sequence, with each model dependent on the previous ones.   \n",
    "# Models are weighted based on their performance.   \n",
    "# Examples: AdaBoost, Gradient Boosting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. What are the benefits of using ensemble techniques?\n",
    "\n",
    "# Ans:\n",
    "\n",
    "# Benifits:\n",
    "# Improve accuracy\n",
    "# Reduce overfitting\n",
    "# Handling complex problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. Are ensemble techniques always better than individual models?\n",
    "\n",
    "# Ans:\n",
    "\n",
    "# While ensemble techniques often provide significant improvements in accuracy and robustness compared \n",
    "# to individual models, they are not always better. There are situations where using an individual model \n",
    "# might be preferable or more appropriate.\n",
    "\n",
    "# When Ensemble Techniques Might Not Be the Best Choice:\n",
    "\n",
    "# Simple Problems: For very simple problems with clear patterns in the data, a single, well-chosen model \n",
    "# might be sufficient and easier to interpret. Ensemble techniques might add unnecessary complexity.\n",
    "# Limited Computational Resources: Ensemble methods, especially those with many models or complex base learners, \n",
    "# can be computationally expensive to train and deploy. If resources are limited, a single model might \n",
    "# be more practical.   \n",
    "# Interpretability: Ensemble models, especially those with many diverse components, can be harder to interpret\n",
    "# than individual models. If understanding the model's reasoning is crucial, a simpler model might be preferred.   \n",
    "# Real-time Constraints: In situations with strict real-time constraints, the overhead of combining multiple \n",
    "# models might be too high. A single, fast model might be necessary.   \n",
    "# Small Datasets: With very small datasets, the benefits of ensemble techniques might be limited, \n",
    "# and the added complexity might not be justified.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. How is the confidence interval calculated using bootstrap?\n",
    "\n",
    "# Ans:\n",
    "\n",
    "# The bootstrap method is a powerful technique for estimating the confidence interval of a statistic \n",
    "# (like the mean, median, or standard deviation) when the underlying distribution of the data is unknown or complex.\n",
    "\n",
    "# 1. Resampling:\n",
    "# Start with the original dataset, which we'll call the \"original sample.\"   \n",
    "# Create a new dataset by randomly sampling with replacement from the original sample. This means that we \n",
    "# can pick the same data point multiple times for the new dataset.   \n",
    "# This new dataset is called a \"bootstrap sample\". It's like taking a new sample from the original population,\n",
    "# but since we don't know the true population, we use the original sample as a proxy.   \n",
    "\n",
    "# 2. Calculating the Statistic:\n",
    "# Calculate the statistic of interest (e.g. the mean) for the bootstrap sample.   \n",
    "\n",
    "# 3. Repeating:\n",
    "# Repeat steps 1 and 2 many times (typically thousands of times), creating a large number of bootstrap samples \n",
    "# and calculating the statistic for each one.   \n",
    "\n",
    "# 4. Constructing the Confidence Interval:\n",
    "# Once we have a collection of the statistic values from all the bootstrap samples, we can construct the \n",
    "# confidence interval. There are a couple of common ways to do this:\n",
    "    # Percentile Method: Sort the statistic values from all the bootstrap samples in ascending order. \n",
    "    # To get a 95% confidence interval, find the 2.5th percentile and the 97.5th percentile of these values. \n",
    "    # These two values will be the lower and upper bounds of your confidence interval.   \n",
    "    # Normal Approximation Method: If the distribution of the bootstrap statistics looks approximately \n",
    "    # normal, we can use the mean and standard deviation of the bootstrap statistics to calculate the \n",
    "    # confidence interval using the standard formula for a normal distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8. How does bootstrap work and What are the steps involved in bootstrap?\n",
    "\n",
    "# Ans:\n",
    "\n",
    "# Example\n",
    "\n",
    "# Let's say you have the following data points: 2, 4, 6, 8, 10. You want to estimate the 95% confidence interval for the mean.\n",
    "\n",
    "# Resample: You create a bootstrap sample, for example: 4, 6, 6, 10, 2.\n",
    "# Calculate: The mean of this bootstrap sample is 5.6.\n",
    "# Repeat: You repeat steps 1 and 2 many times, getting lots of means.   \n",
    "# Confidence interval: You sort all the means and find the 2.5th and 97.5th percentiles. These are the bounds of your 95% confidence interval.\n",
    "# Key Points\n",
    "\n",
    "# Bootstrap is great when you don't know the underlying distribution of your data.   \n",
    "# It's useful for estimating confidence intervals and standard errors.   \n",
    "# The more bootstrap samples you create, the more accurate your estimates will be.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% Confidence Interval: (14.03, 15.06) meters\n"
     ]
    }
   ],
   "source": [
    "# Q9. A researcher wants to estimate the mean height of a population of trees. They measure the height of a\n",
    "# sample of 50 trees and obtain a mean height of 15 meters and a standard deviation of 2 meters. Use\n",
    "# bootstrap to estimate the 95% confidence interval for the population mean height.\n",
    "\n",
    "# Ans:\n",
    "\n",
    "# Sample size (n) = 50\n",
    "# Sample mean = 15 meters\n",
    "# Sample standard deviation = 2 meters\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "np.random.seed(42)  \n",
    "original_sample = np.random.normal(loc=15, scale=2, size=50) #Simulating the original sample\n",
    "\n",
    "n_bootstraps = 10000  # Number of bootstrap resamples\n",
    "bootstrap_means = []\n",
    "\n",
    "for _ in range(n_bootstraps):\n",
    "    # Resample with replacement\n",
    "    bootstrap_sample = np.random.choice(original_sample, size=50, replace=True)\n",
    "    \n",
    "    # Calculate the mean of the bootstrap sample\n",
    "    bootstrap_mean = np.mean(bootstrap_sample)\n",
    "    bootstrap_means.append(bootstrap_mean)\n",
    "\n",
    "# Calculate the 95% confidence interval using percentiles\n",
    "alpha = 0.05\n",
    "lower_bound = np.percentile(bootstrap_means, 100 * alpha / 2)\n",
    "upper_bound = np.percentile(bootstrap_means, 100 * (1 - alpha / 2))\n",
    "\n",
    "print(f\"95% Confidence Interval: ({lower_bound:.2f}, {upper_bound:.2f}) meters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
